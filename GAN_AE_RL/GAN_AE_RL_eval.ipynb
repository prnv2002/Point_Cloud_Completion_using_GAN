{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.episodes = []\n",
    "        self.buffer_size = size\n",
    "\n",
    "    def add_to_buffer(self, state, action, reward, next_state):\n",
    "        if len(self.episodes) == self.buffer_size:\n",
    "            self.episodes = self.episodes[1:]\n",
    "        self.episodes.append((state.detach().cpu().numpy(), action.detach().cpu().numpy(), reward.detach().cpu().numpy(), next_state.detach().cpu().numpy()))\n",
    "\n",
    "    def get_batch(self, batch_size=10):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_state = []\n",
    "        done = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            epi = random.choice(self.episodes)\n",
    "            states.append(epi[0])\n",
    "            actions.append(epi[1])\n",
    "            rewards.append(epi[2])\n",
    "            next_state.append(epi[3])\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "        rewards = rewards.reshape((rewards.shape[0],1))\n",
    "        return torch.Tensor(states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, z_shape):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = z_shape\n",
    "        \n",
    "\n",
    "        self.linear1 = nn.Linear(self.state_dim, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(400)\n",
    "        self.linear2 = nn.Linear(400 + z_shape, 300)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.linear3 = nn.Linear(300, 300)\n",
    "        self.linear4 = nn.Linear(300, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, state, z):\n",
    "        out = (F.relu(self.linear1(state)))\n",
    "        out = (F.relu(self.linear2(torch.cat([out, z], dim=1))))\n",
    "        out = self.linear3(out)\n",
    "        out = self.linear4(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_dim,  z_shape, max_action=10):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = z_shape\n",
    "\n",
    "        self.linear1 = nn.Linear(self.state_dim, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.linear2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "\n",
    "        self.linear3 = nn.Linear(400, 300)\n",
    "        self.linear4 = nn.Linear(300, self.num_actions)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = F.leaky_relu((self.linear1(x)))\n",
    "        out = F.leaky_relu((self.linear2(out)))\n",
    "        out = torch.tanh(self.linear3(out))\n",
    "        out = self.max_action * torch.tanh(self.linear4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        conv1 = [nn.Conv1d(3, 64, kernel_size=1),\n",
    "                 nn.BatchNorm1d(64),\n",
    "                 nn.ReLU()]\n",
    "        conv2 = [nn.Conv1d(64, 128, kernel_size=1),\n",
    "                 nn.BatchNorm1d(128),\n",
    "                 nn.ReLU()]\n",
    "        conv3 = [nn.Conv1d(128, 256, kernel_size=1),\n",
    "                 nn.BatchNorm1d(256),\n",
    "                 nn.ReLU()]\n",
    "        conv4 = [nn.Conv1d(256, 128, kernel_size=1),\n",
    "                 nn.BatchNorm1d(128),\n",
    "                 nn.AdaptiveMaxPool1d(1)]\n",
    "        self.conv1 = nn.Sequential(*conv1)\n",
    "        self.conv2 = nn.Sequential(*conv2)\n",
    "        self.conv3 = nn.Sequential(*conv3)\n",
    "        self.conv4 = nn.Sequential(*conv4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv1(x)\n",
    "        out_2 = self.conv2(out_1)\n",
    "        out_3 = self.conv3(out_2)\n",
    "        out_4 = self.conv4(out_3)\n",
    "        out_4 = out_4.view(-1, out_4.shape[1])\n",
    "        return out_4\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(Decoder, self).__init__()\n",
    "        linear1 = [nn.Linear(128, 256),\n",
    "                   nn.BatchNorm1d(256),\n",
    "                   nn.ReLU()]\n",
    "        linear2 = [nn.Linear(256, 256),\n",
    "                   nn.BatchNorm1d(256),\n",
    "                   nn.ReLU()]\n",
    "        linear3 = [nn.Linear(256, 6144),\n",
    "                   nn.ReLU()]\n",
    "        self.linear1 = nn.Sequential(*linear1)\n",
    "        self.linear2 = nn.Sequential(*linear2)\n",
    "        self.linear3 = nn.Sequential(*linear3)\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.linear1(x)\n",
    "        out_2 = self.linear2(out_1)\n",
    "        out_3 = self.linear3(out_2)\n",
    "\n",
    "        return out_3.view(-1, 3, self.num_points)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(num_points)\n",
    "\n",
    "    def encode(self, x):\n",
    "        gfv = self.encoder(x)\n",
    "        return gfv\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class ChamferLoss(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(ChamferLoss, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.loss = torch.FloatTensor([0]).to(device)\n",
    "\n",
    "    def forward(self, predict_pc, gt_pc):\n",
    "        z, _ = torch.min(torch.norm(gt_pc.unsqueeze(-2) -\n",
    "                                    predict_pc.unsqueeze(-1), dim=1), dim=-2)\n",
    "        self.loss = z.sum() / (len(gt_pc)*(gt_pc.shape[2]+predict_pc.shape[2]))\n",
    "\n",
    "        z_2, _ = torch.min(torch.norm(\n",
    "            predict_pc.unsqueeze(-2) - gt_pc.unsqueeze(-1), dim=1), dim=-2)\n",
    "        self.loss += z_2.sum() / (len(gt_pc)*(gt_pc.shape[2]+predict_pc.shape[2]))\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAttn(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SAttn, self).__init__()\n",
    "\n",
    "        self.query = nn.Conv2d(dim, dim // 8, 1)\n",
    "        self.key = nn.Conv2d(dim, dim//8, 1)\n",
    "        self.value = nn.Conv2d(dim, dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, w, h = x.size()\n",
    "        query = self.query(x)\n",
    "        query = query.view(batch_size, -1, w*h).permute(0, 2, 1)\n",
    "        key = self.key(x).view(batch_size, -1, w*h)\n",
    "        \n",
    "        matmul = torch.bmm(query, key)\n",
    "        attn = self.softmax(matmul)\n",
    "\n",
    "        value = self.value(x).view(batch_size, -1, w*h)\n",
    "\n",
    "        out = torch.bmm(value, attn.permute(0,2,1))\n",
    "        out = out.view(batch_size, c, w, h)\n",
    "        out = self.gamma*out + x\n",
    "\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenSAGAN(nn.Module):\n",
    "    def __init__(self, image_size=32, z_dim=32, conv_dim=64):\n",
    "        super(GenSAGAN, self).__init__()\n",
    "        repeat_num = int(np.log2(image_size)) - 3\n",
    "        mult = 2 ** repeat_num\n",
    "\n",
    "        self.layer1 = nn.ConvTranspose2d(z_dim, conv_dim*mult, 4)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_dim*mult)\n",
    "\n",
    "        self.layer2 = nn.ConvTranspose2d(conv_dim*mult, (conv_dim*mult)//2, 3, 2, 2)\n",
    "        self.bn2 = nn.BatchNorm2d((conv_dim*mult)//2)\n",
    "\n",
    "        self.layer3 = nn.ConvTranspose2d((conv_dim*mult)//2, (conv_dim*mult)//4, 3, 2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d((conv_dim*mult)//4)\n",
    "\n",
    "\n",
    "        self.layer4 = nn.ConvTranspose2d(64, 1, 2, 2, 1)\n",
    "\n",
    "        self.attn1 = SAttn(64)\n",
    "        self.attn2 = SAttn(64)\n",
    "\n",
    "        self.conv1d = nn.ConvTranspose1d(144, 128, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], x.shape[1], 1, 1)\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = F.relu(self.layer3(out))\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        \n",
    "        out ,  p1 = self.attn1(out)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = out.view(-1, 1, 144)\n",
    "        out = out.transpose(1, 2)\n",
    "\n",
    "        out = self.conv1d(out)\n",
    "        out = out.transpose(2, 1)\n",
    "\n",
    "        out = out.view(-1, 128)\n",
    "\n",
    "        return out , p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscSAGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size=32, conv_dim=64):\n",
    "        super(DiscSAGAN, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(1, conv_dim, 3, 2, 2)\n",
    "        self.layer2 = nn.Conv2d(conv_dim, conv_dim*2, 3, 2, 2)\n",
    "        self.layer3 = nn.Conv2d(conv_dim*2, conv_dim*4, 3 ,2, 2)\n",
    "\n",
    "        self.layer4 = nn.Conv2d(conv_dim*4, 1, 4)\n",
    "\n",
    "        self.attn1 = SAttn(256)\n",
    "        self.attn2 = SAttn(512)\n",
    "\n",
    "        self.conv1d = nn.ConvTranspose1d(128, 144, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.squeeze(1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = x.view(-1, 1, 12, 12)\n",
    "\n",
    "        out = F.leaky_relu(self.layer1(x))\n",
    "        out = F.leaky_relu(self.layer2(out))\n",
    "        out = F.leaky_relu(self.layer3(out))\n",
    "\n",
    "        out, p1 = self.attn1(out)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "        out = out.reshape(x.shape[0], -1)\n",
    "        return out, p1\n",
    "\n",
    "class DDPG(nn.Module):\n",
    "    def __init__(self, max_action):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.actor = ActorNet(128, z_dim, max_action)\n",
    "        self.critic = CriticNet(128, z_dim)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "    def get_optimal_action(self, state):\n",
    "        return self.actor(state)\n",
    "\n",
    "    def forward(self):\n",
    "        state, action, reward, next_state = self.replay_buffer.get_batch(batch_size) #get_batch(batch_size_actor)\n",
    "        \n",
    "        state = state[:,0,:].float()\n",
    "        next_state = next_state[:,0,:].float()\n",
    "        action = action[:,0,:].float()\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        \n",
    "        target_q = reward\n",
    "\n",
    "        q_batch = self.critic(state, action)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        value_loss = F.mse_loss(q_batch, target_q)\n",
    "        value_loss.backward()\n",
    "        \n",
    "        self.critic_optimizer.step() \n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "\n",
    "        policy_loss = - self.critic(state, self.actor(state)).mean()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ae = '../Load/AE_weights/990_ae_.pt'\n",
    "weights_gen = '../Load/GAN_weights/980_gen_.pt'\n",
    "weights_disc = '../Load/GAN_weights/980_disc_.pt'\n",
    "weight_ddpg = '../Load/GAN_AE_RL_weights/335000_ddpg_.pt'\n",
    "\n",
    "max_action = 2\n",
    "z_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(2048).to(device)\n",
    "autoencoder.load_state_dict(torch.load(weights_ae, map_location=torch.device('cpu')))\n",
    "\n",
    "generator = GenSAGAN(z_dim=z_dim).to(device)\n",
    "generator.load_state_dict(torch.load(weights_gen, map_location=torch.device('cpu')))\n",
    "\n",
    "discriminator = DiscSAGAN().to(device)\n",
    "discriminator.load_state_dict(torch.load(weights_disc, map_location=torch.device('cpu')))\n",
    "\n",
    "ddpg = DDPG(max_action).to(device)\n",
    "ddpg.load_state_dict(torch.load(weight_ddpg, map_location=torch.device('cpu')))\n",
    "\n",
    "autoencoder.eval()\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "DATA_DIR = '../latent_3d_points/data/shape_net_core_uniform_samples_2048/'\n",
    "list_point_clouds = np.load('../Load/numpy_arrays/list_point_noisy.npy')\n",
    "list_point_clouds = list_point_clouds[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, _, _ = train_test_split(list_point_clouds, list_point_clouds, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointcloudDatasetAE(Dataset):\n",
    "    def __init__(self, root, list_point_clouds):\n",
    "        self.root = root\n",
    "        self.list_files = list_point_clouds\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        points = PyntCloud.from_file(self.list_files[index])\n",
    "        points = np.array(points.points)\n",
    "        points_normalized = (points - (-0.5)) / (0.5 - (-0.5))\n",
    "        points = points_normalized.astype(np.float64)\n",
    "        points = torch.from_numpy(points)\n",
    "        \n",
    "        return points\n",
    "\n",
    "class PointcloudDatasetNoisy(Dataset):\n",
    "    def __init__(self, root, list_point_clouds):\n",
    "        self.root = root\n",
    "        self.list_files = list_point_clouds\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        points = self.list_files[index]\n",
    "        # points = np.array(points.points)\n",
    "        points_normalized = (points - (-0.5)) / (0.5 - (-0.5))\n",
    "        points = points_normalized.astype(np.float64)\n",
    "        points = torch.from_numpy(points)\n",
    "        \n",
    "        return points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './GAN_AE_RL_results/'\n",
    "now =   str(datetime.datetime.now())\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.makedirs(ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR + now):\n",
    "    os.makedirs(ROOT_DIR + now)\n",
    "\n",
    "OUTPUTS_DIR = ROOT_DIR  + now + '/outputs/'\n",
    "if not os.path.exists(OUTPUTS_DIR):\n",
    "    os.makedirs(OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chamferloss = ChamferLoss(2048).to(device)\n",
    "\n",
    "train_dataset = PointcloudDatasetNoisy(DATA_DIR, X_train)\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=0, shuffle=True, batch_size=1)\n",
    "\n",
    "test_dataset = PointcloudDatasetNoisy(DATA_DIR, X_test)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=0, shuffle=True, batch_size=1)\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    data = data.permute([0,2,1])\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    data = data.permute([0,2,1]).float().to(device)\n",
    "            \n",
    "    state_t = autoencoder.encode(data)\n",
    "\n",
    "    optimal_action = ddpg.get_optimal_action(state_t).detach()\n",
    "    new_state, _ = generator(optimal_action)\n",
    "    \n",
    "    out_data = autoencoder.decode(new_state)\n",
    "\n",
    "    output = out_data[0,:,:]\n",
    "    output = output.permute([1,0]).detach().cpu().numpy()\n",
    "\n",
    "    inputt = data[0,:,:]\n",
    "    inputt = inputt.permute([1,0]).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax_x = fig.add_subplot(111, projection='3d')\n",
    "    x_ = inputt\n",
    "    ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "    ax_x.set_xlim([0,1])\n",
    "    ax_x.set_ylim([0,1])\n",
    "    ax_x.set_zlim([0,1])\n",
    "    fig.savefig(OUTPUTS_DIR+'/{}_{}.png'.format(i, 'in'))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax_x = fig.add_subplot(111, projection='3d')\n",
    "    x_ = output\n",
    "    ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "    ax_x.set_xlim([0,1])\n",
    "    ax_x.set_ylim([0,1])\n",
    "    ax_x.set_zlim([0,1])\n",
    "    fig.savefig(OUTPUTS_DIR+'/{}_{}_{}.png'.format(i, 'rl_out'))\n",
    "\n",
    "    output = autoencoder.decode(state_t)\n",
    "    output = output[0,:,:]\n",
    "    output = output.permute([1,0]).detach().cpu().numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax_x = fig.add_subplot(111, projection='3d')\n",
    "    x_ = output\n",
    "    ax_x.scatter(x_[:, 0], x_[:, 1], x_[:,2])\n",
    "    ax_x.set_xlim([0,1])\n",
    "    ax_x.set_ylim([0,1])\n",
    "    ax_x.set_zlim([0,1])\n",
    "    fig.savefig(OUTPUTS_DIR+'/{}_{}_{}.png'.format(i, 'ae_out'))\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "    if i > 2:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
